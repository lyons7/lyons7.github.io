---
date: 2015-07-23T21:13:14-05:00
draft: false
image: "img/portfolio/tidytext.jpg"
title: "Having Fun with Tidy Text!"
summary: "Some applications of Silge and Robinson's 'Text Mining with R' examples"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Julia Silge and David Robinson have a wonderful new book called "Text Mining with R" which has a [companion website](http://tidytextmining.com/) with great explanations and examples. Here are some additional applications of those examples on a corpus of geotagged Instagram posts from the Mission District neighborhood in San Francisco.  

Let's have some fun with tidy text!

```{r, echo=TRUE, warning=FALSE, message=FALSE, error=FALSE, results='hide'}
# Make sure you have the right packages!
packs = c("twitteR","RCurl","RJSONIO","stringr","ggplot2","devtools","DataCombine","ggmap",
          "topicmodels","slam","Rmpfr","tm","stringr","wordcloud","plyr",
          "tidytext","dplyr","tidyr","xlsx","scales","ggrepel","lubridate","purrr","broom")
lapply(packs, library, character.only=T)
```


```{r, echo = FALSE}
tweets=read.csv("Col_Sep_INSTACORPUS.csv", header=T)
```

The data need to be processed a bit more in order to analyze them. Let's try from the start with [Silge and Robinson](http://tidytextmining.com/).

```{r, echo=TRUE,warning=FALSE,message=FALSE}
# Get rid of stuff particular to the data (here encodings of links and such)
# Most of these are characters I don't have encodings for (other scripts, etc.)
tweets$text = gsub("Just posted a photo","", tweets$text)
tweets$text = gsub( "<.*?>", "", tweets$text)

# Get rid of super frequent spam posters
tweets <- tweets[! tweets$screenName %in% c("4AMSOUNDS",
      "BruciusTattoo","LionsHeartSF","hermesalchemist","Mrsourmash"),]


# Now for Silge and Robinson's code. What this is doing is getting rid of URLs, re-tweets (RT) and ampersands. This also gets rid of stop words without having to get rid of hashtags and @ signs by using str_detect and filter! 
reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_tweets <- tweets %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
```

Awesome! Now our posts are cleaned with the hashtags and @ mentions still intact. What we can try now is to plot the frequency of some of these terms according to WHERE they occur. Silge and Robinson have an example with persons, let's try with coordinates. 

```{r, echo=TRUE, message=FALSE, warning=FALSE}
freq <- tidy_tweets %>% 
  group_by(latitude,longitude) %>% 
  count(word, sort = TRUE) %>% 
  left_join(tidy_tweets %>% 
              group_by(latitude,longitude) %>% 
              summarise(total = n())) %>%
  mutate(freq = n/total)

freq
```

The n here is the total number of times this term has shown up, and the total is how many terms there are present in a particular coordinate. 

Cool! Now we have a representation of terms, their frequency and their position. Now I might want to plot this somehow... one way would be to try to plot the most frequent terms (n > 50)
(Some help on how to do this was taken from [here](http://blog.revolutionanalytics.com/2016/01/avoid-overlapping-labels-in-ggplot2-charts.html) and [here](http://stackoverflow.com/questions/14288001/geom-text-not-working-when-ggmap-and-geom-point-used)).

```{r, echo=TRUE, message=FALSE, warning=FALSE}
freq2 <- subset(freq, n > 50) 

map <- get_map(location = 'Valencia St. and 20th, San Francisco,
               California', zoom = 15)

freq2$longitude<-as.numeric(freq2$longitude)
freq2$latitude<-as.numeric(freq2$latitude)
lon <- freq2$longitude
lat <- freq2$latitude

mapPoints <- ggmap(map) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_label_repel(data = freq2, aes(x = lon, y = lat, label = word),size = 2) 
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
mapPoints
```

Let's zoom into that main central area to see what's going on!

```{r, echo=TRUE, message=FALSE, warning=FALSE}
map2 <- get_map(location = 'Lexington St. and 19th, San Francisco,
               California', zoom = 16)
mapPoints2 <- ggmap(map2) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_label_repel(data = freq2, aes(x = lon, y = lat, label = word),size = 2) 
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
mapPoints2
```

This can be manipulated in many different ways -- either by playing with what frequency of terms you want to look at (maybe I want to see terms that occur 100 times, between 20 and 50 times, less than 20 times etc. etc.) OR by playing around with the map. At the moment though, this is pretty illuminating in the sense that it shows us that the most frequency terms are focused around certain 'hotspots' in the area, which in itself is just kind of cool to see. 

Now let's try out word frequency changes over time: what words were used more or less over the time of data collection? (Help from [here](http://tidytextmining.com/twitter.html))
(Also used the [lubridate package](https://cran.r-project.org/web/packages/lubridate/lubridate.pdf) to help with time.)

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Might have to do this first
tidy_tweets$created2 <- as.POSIXct(tidy_tweets$created, format="%m/%d/%Y %H:%M")

words_by_time <- tidy_tweets %>%
  mutate(time_floor = floor_date(created2, unit = "1 week")) %>%
  count(time_floor, word) %>%
  ungroup() %>%
  group_by(time_floor) %>%
  mutate(time_total = sum(n)) %>%
  group_by(word) %>%
  mutate(word_total = sum(n)) %>%
  ungroup() %>%
  rename(count = n) %>%
  filter(word_total > 100)

words_by_time
```

Alright, now we want to figure out those words that have changed the most in their frequency over time so as to isolate ones of interests to plot over time. This involves a few steps though.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
nested_data <- words_by_time %>%
  nest(-word) 
nested_data
```

Process as described by Silge and Robinson: "This data frame has one row for each person-word combination; the data column is a list column that contains data frames, one for each combination of person and word. Let’s use map() from the purrr library to apply our modeling procedure to each of those little data frames inside our big data frame. This is count data so let’s use glm() with family = "binomial" for modeling. We can think about this modeling procedure answering a question like, “Was a given word mentioned in a given time bin? Yes or no? How does the count of word mentions depend on time?”"

```{r, echo=TRUE, message=FALSE, warning=FALSE}
nested_models <- nested_data %>%
  mutate(models = map(data, ~ glm(cbind(count, time_total) ~ time_floor, ., 
                                  family = "binomial")))

nested_models
```

Silge and Robinson: "Now notice that we have a new column for the modeling results; it is another list column and contains glm objects. The next step is to use map() and tidy() from the broom package to pull out the slopes for each of these models and find the important ones. We are comparing many slopes here and some of them are not statistically significant, so let’s apply an adjustment to the p-values for multiple comparisons."

```{r}
slopes <- nested_models %>%
  unnest(map(models, tidy)) %>%
  filter(term == "time_floor") %>%
  mutate(adjusted.p.value = p.adjust(p.value))
```

"Now let’s find the most important slopes. Which words have changed in frequency at a moderately significant level in our tweets?"

```{r}
top_slopes <- slopes %>% 
  filter(adjusted.p.value < 0.1)

top_slopes
```

Let's plot them!

```{r}
words_by_time %>%
  inner_join(top_slopes, by = c("word")) %>%
  ggplot(aes(time_floor, count/time_total, color = word)) +
  geom_line(size = 1.3) +
  labs(x = NULL, y = "Word frequency")
```

After looking at some of these features of our data set, it's time to explore TOPIC MODELING, or (paraphrasing from David Blei) finding structure in more-or-less unstructured documents. To do this we need a document-term matrix. At the moment, the tweets are a little problematic in that they are broken up by words... whereas we actually would like the text of the tweet back as that is what we are treating as our 'document'. The question at the moment is... do we want to keep the hashtags / can we? 

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Let's try by taking our tweets that have been tidied already. First we need to count each word though, and create some kind of column that has 
# This first one is helpful for seeing encodings that need to be removed
tidy_tweets %>%
  count(document, word, sort=TRUE)

# Counting words so we can make a dtm with our preserved corpus with hashtags and such
tweet_words <- tidy_tweets %>%  
  count(document, word) %>%
  ungroup()

total_words <- tweet_words %>% 
  group_by(document) %>% 
  summarize(total = sum(n))

post_words <- left_join(tweet_words, total_words)

post_words

new_dtm <- post_words %>% 
  cast_dtm(document, word, n)
```

This seems to have worked :O :O :O
Let's see how topic modeling works here now...

Visualization in TIDY form also from [Silge and Robinson](http://tidytextmining.com/topicmodeling.html)!

```{r, echo=TRUE, message=FALSE, warning=FALSE}
#Set parameters for Gibbs sampling (parameters those used in
#Grun and Hornik 2011)
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
k <- 12
test_lda2 <-LDA(new_dtm,k, method="Gibbs", 
             control=list(nstart=nstart, seed = seed, best=best, 
                          burnin = burnin, iter = iter, thin=thin))

# Make that TIDY!!! 
test_lda_td2 <- tidy(test_lda2)

lda_top_terms2 <- test_lda_td2 %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

lda_top_terms2 %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```


Still working on connecting assigned topics back to the tweets from whence they came, but as soon as I figure that out I'll add it! ;)