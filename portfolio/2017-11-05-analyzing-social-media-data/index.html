<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>Intro to Analyzing Social Media Data</title>
<meta name="description" content="Describe your website">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="robots" content="all,follow">
<meta name="googlebot" content="index,follow,snippet,archive">
<link rel="stylesheet" href="/css/bootstrap.min.css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:400,300,700,400italic">
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/owl.carousel.css">
<link rel="stylesheet" href="/css/owl.theme.css">


  <link href="/css/style.default.css" rel="stylesheet" id="theme-stylesheet">


<link href="/css/custom.css" rel="stylesheet">
<link rel="shortcut icon" href="/img/favicon.png">


</head>
<body>
  <div id="all">
      <div class="container-fluid">
          <div class="row row-offcanvas row-offcanvas-left">
              <div id="sidebar" class="col-xs-6 col-sm-4 col-md-3 sidebar-offcanvas">
  <div class="sidebar-content">
    <h1 class="sidebar-heading"><a href="/">Kate Lyons</a></h1>
    
      <p class="sidebar-p">Graduate student in Linguistics at UIUC</p>
    
    <ul class="sidebar-menu">
      
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/about/">About</a></li>
      
        <li><a href="/contact/">Get in touch</a></li>
      
    </ul>
    <p class="social">
  
  
  
  <a href="https://twitter.com/katester117" data-animate-hover="pulse" class="external twitter">
    <i class="fa fa-twitter"></i>
  </a>
  
  
  
  <a href="mailto:kalyons3@illinois.edu" data-animate-hover="pulse" class="email">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  
  <a href="http://stackoverflow.com/users/7451811/kate" data-animate-hover="pulse">
    <i class="fa fa-stack-overflow"></i>
  </a>
  
  
  <a href="https://github.com/lyons7" data-animate-hover="pulse">
    <i class="fa fa-github"></i>
  </a>
  
</p>


    <div class="copyright">
      <p class="credit">
        
          &copy;2017 Kate Lyons
        
        | Template by <a href="https://bootstrapious.com/free-templates" class="external">Bootstrapious.com</a>

&amp; ported to Hugo by <a href="https://github.com/kishaningithub">Kishan B</a>

      </p>
    </div>
  </div>
</div>

              
<div class="col-xs-12 col-sm-8 col-md-9 content-column white-background">
  <div class="small-navbar visible-xs">
  <button type="button" data-toggle="offcanvas" class="btn btn-ghost pull-left"> <i class="fa fa-align-left"> </i>Menu</button>
  <h1 class="small-navbar-heading"><a href="/">Kate Lyons</a></h1>
</div>

  <div class="row">
    <div class="col-lg-8">
      <div class="content-column-content">
         <h1>Intro to Analyzing Social Media Data</h1>
         <p>Today we are going to talk about applying text mining techniques to social media data. You can download this data yourself (if you have everything set up with the Twitter and YouTube APIs) or you can access the data we‚Äôll be discussing here in R data format.</p>
<pre class="r"><code># packs = c(&quot;twitteR&quot;, &quot;stringr&quot;,&quot;ggplot2&quot;,&quot;devtools&quot;,&quot;DataCombine&quot;,&quot;ggmap&quot;,&quot;tm&quot;,&quot;wordcloud&quot;,&quot;plyr&quot;,&quot;tuber&quot;,&quot;tidytext&quot;,&quot;dplyr&quot;,&quot;tidyr&quot;,&quot;readr&quot;,&quot;ggrepel&quot;,&quot;emoGG&quot;,&quot;lubridate&quot;,&quot;corpus&quot;, &quot;purrr&quot;, &quot;broom&quot;)
# lapply(packs, library, character.only=T)

# You might have to install some of these -- check the &#39;Packages&#39; tab in R Studio to see which ones you already have. For &#39;emoGG&#39; you need to download it directly from github, using the following:
# devtools::install_github(&quot;dill/emoGG&quot;)

# Make sure to set the right working directory
# setwd(&quot;/Users/katelyons/Documents/Workshop&quot;)</code></pre>
<div id="twitter-geographic-analyses" class="section level2">
<h2>Twitter + Geographic Analyses</h2>
<p>First we will look at some Twitter data of the Wicker Park neighborhood in Chicago, IL.</p>
<pre class="r"><code># key = &quot;YOUR KEY HERE&quot;
# secret = &quot;YOUR SECRET HERE&quot;

# tok = &quot;YOUR TOK HERE&quot;
# tok_sec = &quot;YOUR TOK_SEC HERE&quot;

# twitter_oauth &lt;- setup_twitter_oauth(key, secret, tok, tok_sec)</code></pre>
<p>If you don‚Äôt have the Twitter API set up, you can access the data in R data format <a href="https://www.dropbox.com/s/nzh8d6fdwcdra9d/sampledata.Rda?dl=0">here</a>. If you get the data this way, start from the ‚ÄòtwListToDF‚Äô step.</p>
<p>Now you have set up your ‚Äòhandshake‚Äô with the API and are ready to collect data. We will search by coordinate for all tweets that have occurred within a 1 kilometer radius of a central point in the Wicker Park neighborhood in Chicago, IL.</p>
<pre class="r"><code># geo &lt;- searchTwitter(&#39;&#39;,n=7000,geocode=&#39;41.908602,-87.677417,1km&#39;,retryOnRateLimit=1)
# Save tweet data (if you want)
# save(geo,file=paste(&quot;sampletweetdata.Rda&quot;))

# If you need to load that data (make sure you are in the right directory)
load(&#39;sampletweetdata.Rda&#39;)

# Convert to data frame
 geoDF&lt;-twListToDF(geo)</code></pre>
<p>Now we have a data frame. We will now identify emojis, select just those tweets that come from Instagram and clean this data (get rid of links, special characters, etc.) so we can do text analyses. You can access the emoji dictionary <a href="https://www.dropbox.com/s/orpj7lmh5ueapo1/Emoji%20Dictionary%202.1.csv?dl=0">here</a>. The code for cleaning the tweets comes from <a href="http://tidytextmining.com/twitter.html">Silge and Robinson‚Äôs book</a> ‚Äì this is special and awesome code to do this because it keeps hashtags and @ mentions. Other methods of cleaning text will count ‚Äò#‚Äô and ‚Äò@‚Äô as special characters and will get rid of them.</p>
<pre class="r"><code># Convert the encoding so you can identify emojis
geoDF$text &lt;- iconv(geoDF$text, from = &quot;latin1&quot;, to = &quot;ascii&quot;, sub = &quot;byte&quot;)

# Load in emoji dictionary. The &#39;trim_ws&#39; argument is super important -- you need those spaces so the emojis aren&#39;t all squished together!
# emojis &lt;- read_csv(&quot;Emoji Dictionary 2.1.csv&quot;, col_names=TRUE, trim_ws=FALSE)
# If you don&#39;t get weird encoding issue just use read.csv
emojis &lt;- read.csv(&quot;Emoji Dictionary 2.1.csv&quot;, header=T)


# Go through and identify emojis
geodata &lt;- FindReplace(data = geoDF, Var = &quot;text&quot;, 
                            replaceData = emojis,
                       from = &quot;R_Encoding&quot;, to = &quot;Name&quot;, 
                       exact = FALSE)

# Just keep those tweets that come from Instagram
wicker &lt;- geodata[geodata$statusSource == 
        &quot;&lt;a href=\&quot;http://instagram.com\&quot; rel=\&quot;nofollow\&quot;&gt;Instagram&lt;/a&gt;&quot;, ]

# Get rid of stuff particular to the data (here encodings of links and such)
# Most of these are characters I don&#39;t have encodings for (other scripts, etc.)
wicker$text = gsub(&quot;Just posted a photo&quot;,&quot;&quot;, wicker$text)
wicker$text = gsub( &quot;&lt;.*?&gt;&quot;, &quot;&quot;, wicker$text)

# Now time to clean your posts. First let&#39;s make our own list of stop words again, adding additional stop words to the tidy text stop word list from the tm package stop word list.
# This makes a larger list of stop words combining those from the tm package and tidy text -- even though the tm package stop word list is pretty small anyway, just doing this just in case
data(stop_words)
mystopwords &lt;- c(stopwords(&#39;english&#39;),stop_words$word)

# Now for Silge and Robinson&#39;s code. What this is doing is getting rid of 
# URLs, re-tweets (RT) and ampersands. This also gets rid of stop words 
# without having to get rid of hashtags and @ signs by using 
# str_detect and filter! 
reg &lt;- &quot;([^A-Za-z_\\d#@&#39;]|&#39;(?![A-Za-z_\\d#@]))&quot;
tidy_wicker &lt;- wicker %&gt;% 
  filter(!str_detect(text, &quot;^RT&quot;)) %&gt;%
  mutate(text = str_replace_all(text, &quot;https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;amp;|&amp;lt;|&amp;gt;|RT|https&quot;, &quot;&quot;)) %&gt;%
  unnest_tokens(word, text, token = &quot;regex&quot;, pattern = reg) %&gt;%
  filter(!word %in% mystopwords,
         str_detect(word, &quot;[a-z]&quot;))</code></pre>
<p>Now we have cleaned posts in tidy format. Let‚Äôs adapt some of <a href="http://tidytextmining.com/twitter.html">Silge and Robinson‚Äôs code</a> to look at frequent terms and then map concentrations of frequent terms. Note how I‚Äôve grouped these by latitude and longitude. This will help us later on when we want to group them by coordinate.</p>
<pre class="r"><code>freq &lt;- tidy_wicker %&gt;% 
  group_by(latitude,longitude) %&gt;% 
  count(word, sort = TRUE) %&gt;% 
  left_join(tidy_wicker %&gt;% 
              group_by(latitude,longitude) %&gt;% 
              summarise(total = n())) %&gt;%
  mutate(freq = n/total)

# Look at most frequent terms
freq</code></pre>
<p>The n here is the total number of times this term has shown up, and the total is how many terms there are present in a particular coordinate.</p>
<p>Cool! Now we have a representation of terms, their frequency and their position. Now I might want to plot this somehow‚Ä¶ one way would be to try to plot the most frequent terms (n &gt; 3) (Some help on how to do this was taken from <a href="http://blog.revolutionanalytics.com/2016/01/avoid-overlapping-labels-in-ggplot2-charts.html">here</a> and <a href="http://stackoverflow.com/questions/14288001/geom-text-not-working-when-ggmap-and-geom-point-used">here</a>). Depending on the size of your data set, the base line for ‚Äòmost frequent‚Äô is subject to change ‚Äì because my data set is relatively small, I‚Äôm just going to say words that appear more than three times.</p>
<pre class="r"><code>freq2 &lt;- subset(freq, n &gt; 3) 

# Let&#39;s get a map!
# This will take a couple tries to make sure you have a nice map for your data
map &lt;- get_map(location = &#39;Damen and North, Chicago, Illinois&#39;, zoom = 14)

freq2$longitude&lt;-as.numeric(freq2$longitude)
freq2$latitude&lt;-as.numeric(freq2$latitude)
lon &lt;- freq2$longitude
lat &lt;- freq2$latitude

mapPoints &lt;- ggmap(map) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_label_repel(data = freq2, aes(x = lon, y = lat, label = word),size = 3) 

mapPoints</code></pre>
<p><img src="/portfolio/2017-11-05-analyzing-social-media-data_files/figure-html/Map%20Tweet%20Frequency-1.png" width="672" /></p>
<p>Now we have a representation of coordinates in which frequent terms are being used. A lot of these are associated with restaurants or bars (see discussion of Piece pizzeria, Emporium Arcade Bar). We can also go back and investigate certain groupings of terms ‚Äì for example, the ‚Äò#inktober‚Äô concentration is from an artist who posts a picture of their art everyday. To check things like the concentration of ‚Äòface with tears of joy‚Äô, we can go back to our original ‚Äòwicker‚Äô data frame and search for the coordinate that is in our freq2 data frame to find the links to the orginal Instagram posts or just search that coordinate on Google maps. For the coordinate where we find lots of ‚Äòface with tears of joy‚Äô emojis, we can see from Google maps that there is a comedy club, ‚ÄòThe Comedy Clubhouse‚Äô at that location ‚Äì guess it‚Äôs a good comedy club!</p>
<p>How about sentiment analysis? What are the most common positive and negative words? This time we‚Äôll just use the sentiment dictionary available in the tidytext package from the BING sentiment corpus.</p>
<pre class="r"><code># We can also look at counts of negative and positive words
bingsenti &lt;- sentiments %&gt;%
  filter(lexicon ==&quot;bing&quot;)

bing_word_counts &lt;- tidy_wicker %&gt;%
  inner_join(bingsenti) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  ungroup()

# bing_word_counts

# Now we can graph these
# Change &#39;filter&#39; parameter depending on the size of your data set
bing_word_counts %&gt;%
  filter(n &gt; 2) %&gt;%
  mutate(n = ifelse(sentiment == &quot;negative&quot;, -n, n)) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_bar(alpha = 0.8, stat = &quot;identity&quot;) +
  labs(y = &quot;Contribution to sentiment&quot;,
       x = NULL) +
  coord_flip()</code></pre>
<p><img src="/portfolio/2017-11-05-analyzing-social-media-data_files/figure-html/Tweet%20Sentiments-1.png" width="672" /></p>
<p>Unsurprisingly, our Instagram posts are mostly positive, with negative sentiments related to cold (Chicago!) or swear words or difficulty. We see how sentiment analysis is not always infallible, as ‚Äòfall‚Äô is counted as negative when really people are talking about the season and see to be happy about it.</p>
<p>How about a word cloud?</p>
<pre class="r"><code>wordcloud(freq$word,freq$n, min.freq=3, 
          colors=brewer.pal(1, &quot;Dark2&quot;))</code></pre>
<p><img src="/portfolio/2017-11-05-analyzing-social-media-data_files/figure-html/Tweet%20Word%20Clouds-1.png" width="672" /></p>
<p>It‚Äôs interesting people talk about Logan Square a lot, but Logan Square is another neighborhood! This could be people talking about Logan Sqaure but tagging a coordinate in Wicker Park or maybe our radius was a little too big and got some Logan Square tweets (it‚Äôs pretty close). And yes, the ‚Äòaubergine‚Äô is the eggplant emoji. And yes, the posts are not about the vegetable.</p>
<p>What if we want to map emojis? We can do this, but it involves a lot of steps and a new package, the <a href="https://github.com/dill/emoGG">‚ÄòemoGG‚Äô package</a>.</p>
<pre class="r"><code># First make a logical vector telling us what posts have emojis and which ones don&#39;t
emogrepl &lt;- grepl(paste(emojis$Name, collapse = &quot;|&quot;), wicker$text)
# Turn this into a data frame so we can merge it with our post data frame
emogreplDF&lt;-as.data.frame(emogrepl)
# Create an id row to merge
wicker$id &lt;- 1:nrow(wicker)
# Do the same for the logical data frame
emogreplDF$id &lt;- 1:nrow(emogreplDF)
# Merge them together
wicker &lt;- merge(wicker,emogreplDF,by=&quot;id&quot;)
# Just keep posts that have emojis in them
emosub &lt;- wicker[wicker$emogrepl == &quot;TRUE&quot;, ]

# Great! Now turn these into tidy format
tidy_emos &lt;- emosub %&gt;% 
  mutate(text = str_replace_all(text, &quot;https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;amp;|&amp;lt;|&amp;gt;|RT|https&quot;, &quot;&quot;)) %&gt;%
  unnest_tokens(word, text, token = &quot;regex&quot;, pattern = reg) %&gt;%
  filter(!word %in% mystopwords,
         str_detect(word, &quot;[a-z]&quot;))

# Have to do this so they will recognize each other to map frequency
emojis$Name &lt;- as.character(emojis$Name)
test &lt;- emojis %&gt;%
  unnest_tokens(word, Name)

# Now just keep emojis, get rid of surrounding text 
emogrepl2 &lt;- grepl(paste(test$word, collapse = &quot;|&quot;), tidy_emos$word)
emogreplDF2&lt;-as.data.frame(emogrepl2)
tidy_emos$id &lt;- 1:nrow(tidy_emos)
emogreplDF2$id &lt;- 1:nrow(emogreplDF2)
checkit &lt;- merge(tidy_emos,emogreplDF2,by=&quot;id&quot;)
emoonly &lt;- checkit[checkit$emogrepl2 == &quot;TRUE&quot;, ]

freqe &lt;- emoonly %&gt;% 
  group_by(latitude,longitude) %&gt;% 
  count(word, sort = TRUE) %&gt;% 
  left_join(emoonly %&gt;% 
              group_by(latitude,longitude) %&gt;% 
              summarise(total = n())) %&gt;%
  mutate(freq = n/total)

print(head(freqe))</code></pre>
<pre><code>## # A tibble: 6 x 6
## # Groups:   latitude, longitude [5]
##   latitude longitude                  word     n total      freq
##      &lt;chr&gt;     &lt;chr&gt;                 &lt;chr&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;
## 1 41.90879 -87.66792    facewithtearsofjoy    12    15 0.8000000
## 2 41.90571 -87.67057 doubleexclamationmark     4     9 0.4444444
## 3 41.90645 -87.67182             aubergine     4     4 1.0000000
## 4  41.9103 -87.67798                  fire     4    14 0.2857143
## 5 41.90879 -87.66792                  eyes     3    15 0.2000000
## 6 41.90974   -87.677             bicyclist     3     7 0.4285714</code></pre>
<pre class="r"><code># Now we have our most common emojis. Note skin color is separate -- you can go into the CSV file to change this if you want but I have them separate so each emoji isn&#39;t counted as a separate thing.

# Map it
freqe2 &lt;- subset(freqe, n &gt; 2) 

freqe2$longitude&lt;-as.numeric(freqe2$longitude)
freqe2$latitude&lt;-as.numeric(freqe2$latitude)

mapPoints2 &lt;- ggmap(map) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_label_repel(data = freqe2, aes(x = longitude, y = latitude, label = word),size = 3) 

mapPoints2</code></pre>
<p><img src="/portfolio/2017-11-05-analyzing-social-media-data_files/figure-html/Identify%20Emoji%20Posts%201-1.png" width="672" /></p>
<p>This is more impressive with a larger data set, but you can see the kinds of things you can look at with these tools. Let‚Äôs look at another visualization technique, actually graphing with emojis. Once again, this takes a few steps to set up.</p>
<p>Let‚Äôs do üòÇ ‚Äº üçÜ üî• üëÄ and üîÆ.</p>
<pre class="r"><code># We will use emoGG to find the right code for each one. You can cross-check this with the unicode codepoint listed in your emoji dictionary (usualy they are the same, with all letters in lowercase).
# How to search:
# Identifier for each one for mapping
# &#39;face with tears of joy&#39; 1f602
# &#39;double exclamation mark&#39; 203c
# &#39;aubergine&#39; 1f346
# &#39;fire&#39; 1f525
# &#39;eyes&#39; 1f440
# &#39;crystal ball&#39; 1f52e


tearsofjoygrep &lt;- grepl(paste(&quot; FACEWITHTEARSOFJOY  &quot;), emosub$text)
tearsofjoyDF&lt;-as.data.frame(tearsofjoygrep)
emosub$ID7 &lt;- 1:nrow(emosub)
tearsofjoyDF$ID7 &lt;- 1:nrow(tearsofjoyDF)
emosub &lt;- merge(emosub,tearsofjoyDF,by=&quot;ID7&quot;)
tearsofjoy &lt;- emosub[emosub$tearsofjoygrep == &quot;TRUE&quot;, ]

doubleexgrep &lt;- grepl(paste(&quot; DOUBLEEXCLAMATIONMARK  &quot;), emosub$text)
doubleexDF&lt;-as.data.frame(doubleexgrep)
doubleexDF$ID7 &lt;- 1:nrow(doubleexDF)
emosub &lt;- merge(emosub,doubleexDF,by=&quot;ID7&quot;)
doubleex &lt;- emosub[emosub$doubleexgrep == &quot;TRUE&quot;, ]

egggrep &lt;- grepl(paste(&quot; AUBERGINE  &quot;), emosub$text)
eggDF &lt;-as.data.frame(egggrep)
eggDF$ID7 &lt;- 1:nrow(eggDF)
emosub &lt;- merge(emosub,eggDF,by=&quot;ID7&quot;)
aubergine &lt;- emosub[emosub$egggrep == &quot;TRUE&quot;, ]

firegrep &lt;- grepl(paste(&quot; FIRE  &quot;), emosub$text)
fireDF&lt;-as.data.frame(firegrep)
fireDF$ID7 &lt;- 1:nrow(fireDF)
emosub &lt;- merge(emosub,fireDF,by=&quot;ID7&quot;)
fire &lt;- emosub[emosub$firegrep == &quot;TRUE&quot;, ]

eyesgrep &lt;- grepl(paste(&quot; EYES &quot;), emosub$text)
eyesDF&lt;-as.data.frame(eyesgrep)
eyesDF$ID7 &lt;- 1:nrow(eyesDF)
emosub &lt;- merge(emosub,eyesDF,by=&quot;ID7&quot;)
eyes &lt;- emosub[emosub$eyesgrep == &quot;TRUE&quot;, ]

cbgrep &lt;- grepl(paste(&quot; CRYSTALBALL &quot;), emosub$text)
cbDF&lt;-as.data.frame(cbgrep)
cbDF$ID7 &lt;- 1:nrow(cbDF)
emosub &lt;- merge(emosub,cbDF,by=&quot;ID7&quot;)
cb &lt;- emosub[emosub$cbgrep == &quot;TRUE&quot;, ]


# Map this
# Some stuff we have to do first
tearsofjoy$latitude &lt;- as.numeric(tearsofjoy$latitude)
tearsofjoy$longitude &lt;- as.numeric(tearsofjoy$longitude)
doubleex$latitude &lt;- as.numeric(doubleex$latitude)
doubleex$longitude &lt;- as.numeric(doubleex$longitude)
aubergine$latitude &lt;- as.numeric(aubergine$latitude)
aubergine$longitude &lt;- as.numeric(aubergine$longitude)
fire$latitude &lt;- as.numeric(fire$latitude)
fire$longitude &lt;- as.numeric(fire$longitude)
eyes$latitude &lt;- as.numeric(eyes$latitude)
eyes$longitude &lt;- as.numeric(eyes$longitude)
cb$latitude &lt;- as.numeric(cb$latitude)
cb$longitude &lt;- as.numeric(cb$longitude)

# Get a new map that is zoomed in a bit more
map2 &lt;- get_map(location = &#39;Damen and North, Chicago, Illinois&#39;, zoom = 15)


emomap &lt;- ggmap(map2) + geom_emoji(aes(x = longitude, y = latitude), 
                                     data=tearsofjoy, emoji=&quot;1f602&quot;) +
                              geom_emoji(aes(x=longitude, y=latitude),
                                     data=doubleex, emoji=&quot;203c&quot;) +
                              geom_emoji(aes(x=longitude, y=latitude),
                                     data=aubergine, emoji=&quot;1f346&quot;) +
                              geom_emoji(aes(x=longitude, y=latitude),
                                     data=fire, emoji=&quot;1f525&quot;) +
                              geom_emoji(aes(x=longitude, y=latitude),
                                     data=eyes, emoji=&quot;1f440&quot;) +
                              geom_emoji(aes(x=longitude, y=latitude),
                                     data=cb, emoji=&quot;1f52e&quot;)

emomap</code></pre>
<p><img src="/portfolio/2017-11-05-analyzing-social-media-data_files/figure-html/Identify%20Emoji%20Posts-1.png" width="672" /></p>
<p>Again, this is more impressive when you have more data. You can see some examples of this <a href="https://lyons7.github.io/portfolio/2017-03-10-emoji-maps/">here</a>.</p>
</div>
<div id="youtube-diachronic-sentiment-analysis" class="section level2">
<h2>YouTube + Diachronic Sentiment Analysis</h2>
<p>Onwards to YouTube data! We‚Äôll be doing some similar analyses, like sentiment analysis, but instead of looking at things in terms of coordinates, we‚Äôll look at things in terms of time. I wanted to choose something that had a lot of comments, so let‚Äôs look at the comments from the ill-advised (and ill-fated) ‚ÄòEmoji Movie‚Äô trailer. This also has a lot of varying sentiment (one of the comments is ‚ÄúThe movie is a such disgrace to the animation film industry.‚Äùüòπüòπüòπ).</p>
<p>If you don‚Äôt have the YouTube API set up, you can access the data <a href="https://www.dropbox.com/s/3zwmn7vsvo9ihpv/sampletubedata.Rda?dl=0">here</a>.</p>
<pre class="r"><code># Connect to YouTube API
# Leave token blank
# yt_oauth(&quot;app_id&quot;, &quot;app_password&quot;, token=&#39;&#39;)

emojimovie &lt;- get_comment_threads(c(video_id=&quot;o_nfdzMhmrA&quot;), max_results = 101)

# Save data (if you want)
# save(emojimovie,file=paste(&quot;sampletubedata.Rda&quot;))

# If you need to load that data (make sure you are in the right directory)
# load(&#39;sampletubedata.Rda&#39;)</code></pre>
<p>Now we have some (~10,300) comments to play with. Due to the subject matter, emojis are likely to be frequent in our data set, so let‚Äôs follow the same procedure as with our tweets to identify those emojis and label them.</p>
<pre class="r"><code># Convert the encoding so you can identify emojis
# First you have to convert the textOriginal vector into a character so R can go through and identify emojis
emojimovie$text &lt;- as.character(emojimovie$textOriginal)
emojimovie$text &lt;- iconv(emojimovie$text, from = &quot;UTF-8&quot;, to = &quot;ascii&quot;, sub = &quot;byte&quot;)
# Because YouTube encoding is weird
emojimovie$text = gsub(&quot;&lt;f0&gt;&lt;9f&gt;&lt;98&gt;&quot;,&quot;&lt;ed&gt;&lt;a0&gt;&lt;bd&gt;&lt;ed&gt;&lt;b8&gt;&quot;, emojimovie$text)
emojimovie$text = gsub(&quot;&lt;f0&gt;&lt;9f&gt;&lt;8d&gt;&quot;,&quot;&lt;ed&gt;&lt;a0&gt;&lt;bc&gt;&lt;ed&gt;&lt;bd&gt;&quot;, emojimovie$text)
emojimovie$text = gsub(&quot;&lt;f0&gt;&lt;9f&gt;&lt;92&gt;&quot;,&quot;&lt;ed&gt;&lt;a0&gt;&lt;bd&gt;&lt;ed&gt;&lt;b2&gt;&quot;, emojimovie$text)

# Go through and identify emojis
emoemo &lt;- FindReplace(data = emojimovie, Var = &quot;text&quot;, 
                            replaceData = emojis,
                       from = &quot;R_Encoding&quot;, to = &quot;Name&quot;, 
                       exact = FALSE)
# This might take some time, we have a big data set. 


# # Trying to find a more efficient way
# # STILL WORKING ON THIS
# # Another technique to try to fix encoding issue
# # Can it be translated to valid UTF-8 strings?
# # utf8_valid(emojimovie$text)
# # Yes it can...
# # First fix your dictionary to have new YouTube column
# emojis$YouTube &lt;- tolower(emojis$Codepoint)
# emojis$YouTube &lt;- gsub(&quot;u\\+&quot;,&quot;U000&quot;, emojis$YouTube)
# 
# emojimovie$test &lt;- as.character(emojimovie$textOriginal)
# emojimovie$text2 &lt;- utf8_encode(emojimovie$test, display = FALSE)
# # try &lt;- gsub(&quot;\U0001f602&quot;, &quot;lolface&quot;, emojimovie$text2)
# # try[14]
# 
# emojis$Name &lt;- as.factor(emojis$Name)
# emojis$YouTube &lt;- as.factor(emojis$YouTube)
# # Why isn&#39;t this working then?
# emoemo &lt;- FindReplace(data = emojimovie, Var = &quot;text2&quot;,
#                             replaceData = emojis,
#                        from = &quot;YouTube&quot;, to = &quot;Name&quot;,
#                        exact = FALSE)</code></pre>
<p>Now let‚Äôs clean our data using the same code as we used for our tweets (in case there are any hashtags or @ mentions).</p>
<pre class="r"><code>emoemo$text = gsub( &quot;&lt;.*?&gt;&quot;, &quot;&quot;, emoemo$text)

reg &lt;- &quot;([^A-Za-z_\\d#@&#39;]|&#39;(?![A-Za-z_\\d#@]))&quot;
tidy_tube &lt;- emoemo %&gt;% 
  filter(!str_detect(text, &quot;^RT&quot;)) %&gt;%
  mutate(text = str_replace_all(text, &quot;https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;amp;|&amp;lt;|&amp;gt;|RT|https&quot;, &quot;&quot;)) %&gt;%
  unnest_tokens(word, text, token = &quot;regex&quot;, pattern = reg) %&gt;%
  filter(!word %in% mystopwords,
         str_detect(word, &quot;[a-z]&quot;))

freqtube &lt;- tidy_tube %&gt;% 
  group_by(publishedAt) %&gt;% 
  count(word, sort = TRUE) %&gt;% 
  left_join(tidy_tube %&gt;% 
              group_by(publishedAt) %&gt;% 
              summarise(total = n())) %&gt;%
  mutate(freq = n/total)

# Look at most frequent terms
print(head(freqtube))</code></pre>
<pre><code>## # A tibble: 6 x 5
## # Groups:   publishedAt [6]
##                publishedAt               word     n total       freq
##                     &lt;fctr&gt;              &lt;chr&gt; &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;
## 1 2017-10-23T13:00:06.000Z facewithtearsofjoy  2163  2163 1.00000000
## 2 2017-11-05T19:44:54.000Z facewithtearsofjoy  1030  1339 0.76923077
## 3 2017-10-15T17:04:54.000Z                 im   515  2060 0.25000000
## 4 2017-11-05T14:39:16.000Z              movie   515  3193 0.16129032
## 5 2017-10-31T01:15:33.000Z              movie   412  1030 0.40000000
## 6 2017-10-15T21:17:26.000Z               film   309  3708 0.08333333</code></pre>
<pre class="r"><code>bing_word_countstube &lt;- tidy_tube %&gt;%
  inner_join(bingsenti) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  ungroup()

# bing_word_counts

# Now we can graph these
# Change &#39;filter&#39; parameter depending on the size of your data set
bing_word_countstube %&gt;%
  filter(n &gt; 20) %&gt;%
  mutate(n = ifelse(sentiment == &quot;negative&quot;, -n, n)) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_bar(alpha = 0.8, stat = &quot;identity&quot;) +
  labs(y = &quot;Contribution to sentiment&quot;,
       x = NULL) +
  coord_flip()</code></pre>
<p><img src="/portfolio/2017-11-05-analyzing-social-media-data_files/figure-html/YouTube%20Sentiment-1.png" width="672" /></p>
<p>Perhaps unsurprisingly, it seems most of our comments are negative. We can see here some of the biggest contributors to negative sentiment, terms like ‚Äòdisgrace‚Äô or ‚Äòhate‚Äô, ‚Äòcheap‚Äô or ‚Äòcringe‚Äô.</p>
<p>Let‚Äôs look at things over time. Code from <a href="http://tidytextmining.com/twitter.html">Silge and Robinson</a>. This example is looking at the words that have changed the most over time in terms of frequency.</p>
<pre class="r"><code># Need to change some formatting
tidy_tube$created &lt;- as.character(tidy_tube$publishedAt)
tidy_tube$created &lt;- as.POSIXct(tidy_tube$created)

words_by_time &lt;- tidy_tube %&gt;%
  filter(!str_detect(word, &quot;^@&quot;)) %&gt;%
  mutate(time_floor = floor_date(created, unit = &quot;1 day&quot;)) %&gt;%
  count(time_floor, word) %&gt;%
  ungroup() %&gt;%
  group_by(time_floor) %&gt;%
  mutate(time_total = sum(n)) %&gt;%
  group_by(word) %&gt;%
  mutate(word_total = sum(n)) %&gt;%
  ungroup() %&gt;%
  rename(count = n) %&gt;%
  filter(word_total &gt; 30)

# words_by_time

nested_data &lt;- words_by_time %&gt;%
  nest(-word) 

nested_data


nested_models &lt;- nested_data %&gt;%
  mutate(models = map(data, ~ glm(cbind(count, time_total) ~ time_floor, ., 
                                  family = &quot;binomial&quot;)))

nested_models


slopes &lt;- nested_models %&gt;%
  unnest(map(models, tidy)) %&gt;%
  filter(term == &quot;time_floor&quot;) %&gt;%
  mutate(adjusted.p.value = p.adjust(p.value))

top_slopes &lt;- slopes %&gt;% 
  filter(adjusted.p.value &lt; 0.1)

top_slopes

# Graph
words_by_time %&gt;%
  inner_join(top_slopes, by = c(&quot;word&quot;)) %&gt;%
  ggplot(aes(time_floor, count/time_total, color = word)) +
  geom_line(size = 1.3) +
  labs(x = NULL, y = &quot;Word frequency&quot;)</code></pre>
<p><img src="/portfolio/2017-11-05-analyzing-social-media-data_files/figure-html/YouTube%20Comments%20Over%20Time-1.png" width="672" /></p>
<pre class="r"><code># Graph a subset of just slopes of emojis
# First make a logical vector telling us what posts have emojis and which ones don&#39;t
# test &lt;- emojis %&gt;%
#  unnest_tokens(word, Name)
tidy_grep &lt;- grepl(paste(test$word, collapse = &quot;|&quot;), top_slopes$word)

# Turn this into a data frame so we can merge it with our post data frame
tidy_grepDF&lt;-as.data.frame(tidy_grep)
# Create an id row to merge
top_slopes$id &lt;- 1:nrow(top_slopes)
# Do the same for the logical data frame
tidy_grepDF$id &lt;- 1:nrow(tidy_grepDF)
# Merge them together
sub_slopes &lt;- merge(tidy_grepDF,top_slopes,by=&quot;id&quot;)
# Just keep posts that have emojis in them
sub_slopes &lt;- sub_slopes[sub_slopes$tidy_grep == &quot;TRUE&quot;, ]

words_by_time %&gt;%
  inner_join(sub_slopes, by = c(&quot;word&quot;)) %&gt;%
  ggplot(aes(time_floor, count/time_total, color = word)) +
  geom_line(size = 1.3) +
  labs(x = NULL, y = &quot;Word frequency&quot;)</code></pre>
<p><img src="/portfolio/2017-11-05-analyzing-social-media-data_files/figure-html/YouTube%20Comments%20Over%20Time-2.png" width="672" /></p>
<p>What are the most common emojis in comments about the emoji movie?</p>
<pre class="r"><code># First make a logical vector telling us what posts have emojis and which ones don&#39;t
tube_emogrepl &lt;- grepl(paste(emojis$Name, collapse = &quot;|&quot;), emoemo$text)
# Turn this into a data frame so we can merge it with our post data frame
tube_emogreplDF&lt;-as.data.frame(tube_emogrepl)
# Create an id row to merge
emoemo$id &lt;- 1:nrow(emoemo)
# Do the same for the logical data frame
tube_emogreplDF$id &lt;- 1:nrow(tube_emogreplDF)
# Merge them together
emoemo &lt;- merge(emoemo,tube_emogreplDF,by=&quot;id&quot;)
# Just keep posts that have emojis in them
tube_emosub &lt;- emoemo[emoemo$tube_emogrepl == &quot;TRUE&quot;, ]

# Great! Now turn these into tidy format
tidy_tube_emos &lt;- tube_emosub %&gt;% 
  mutate(text = str_replace_all(text, &quot;https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;amp;|&amp;lt;|&amp;gt;|RT|https&quot;, &quot;&quot;)) %&gt;%
  unnest_tokens(word, text, token = &quot;regex&quot;, pattern = reg) %&gt;%
  filter(!word %in% mystopwords,
         str_detect(word, &quot;[a-z]&quot;))

# Now just keep emojis, get rid of surrounding text 
tube_emogrepl2 &lt;- grepl(paste(test$word, collapse = &quot;|&quot;), tidy_tube_emos$word)
tube_emogrepl2DF&lt;-as.data.frame(tube_emogrepl2)
tidy_tube_emos$id &lt;- 1:nrow(tidy_tube_emos)
tube_emogrepl2DF$id &lt;- 1:nrow(tube_emogrepl2DF)
checkit2 &lt;- merge(tidy_tube_emos,tube_emogrepl2DF,by=&quot;id&quot;)
emoonly2 &lt;- checkit2[checkit2$tube_emogrepl2 == &quot;TRUE&quot;, ]

freqe2 &lt;- emoonly2 %&gt;% 
   count(word, sort = TRUE)

print(head(freqe2))</code></pre>
<pre><code>## # A tibble: 6 x 2
##                          word     n
##                         &lt;chr&gt; &lt;int&gt;
## 1          facewithtearsofjoy  3502
## 2                   aubergine   309
## 3                grinningface   309
## 4                      cringe   206
## 5 grinningfacewithsmilingeyes   206
## 6            loudlycryingface   206</code></pre>
<p>So, our most frequent emojis in the comments of the Emoji Movie trailer are üòÇ, üí©, üòä, üòÄ, üòç and üçÜ. Read into that what you will! üòÇ</p>
</div>

         
      </div>
    </div>
  </div>
</div>

          </div>
      </div>
  </div>
  <script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/jquery.cookie.js"> </script>
<script src="/js/ekko-lightbox.js"></script>
<script src="/js/jquery.scrollTo.min.js"></script>
<script src="/js/masonry.pkgd.min.js"></script>
<script src="/js/imagesloaded.pkgd.min.js"></script>
<script src="/js/owl.carousel.min.js"></script>
<script src="/js/front.js"></script>

</body>
</html>
